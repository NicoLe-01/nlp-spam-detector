# -*- coding: utf-8 -*-
"""Proyek NLP Spam Detector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QP0sddKQtwXtXTy2S0_XZZdNpaqxZLh7

Dataset : https://www.kaggle.com/datasets/mfaisalqureshi/spam-email
"""

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

df = pd.read_csv('/content/spam.csv')
df.head()

category = pd.get_dummies(df['Category'])
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Category')
df_baru.head()

email = df_baru['Message'].values
label = df_baru[['ham', 'spam']]

email_train, email_test, label_train, label_test = train_test_split(email, label, test_size=0.2)

tokenizer = Tokenizer(num_words=250, oov_token='-')

tokenizer.fit_on_texts(email_train)
tokenizer.fit_on_texts(email_test)

sekuens_train = tokenizer.texts_to_sequences(email_train)
sekuens_test = tokenizer.texts_to_sequences(email_test)

padded_train = pad_sequences(sekuens_train, maxlen=30)
padded_test = pad_sequences(sekuens_test, maxlen=30)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=250, output_dim=64),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(2, activation='sigmoid')
])

model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

def scheduler(epoch, lr):
  if epoch < 10:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

my_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)

num_epochs = 15
history = model.fit(padded_train, label_train, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2,
                    callbacks=[my_callback])

loss = history.history['loss']
val_loss = history.history['val_loss']
plt.plot(loss)
plt.plot(val_loss)
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train_loss', 'Val_loss'], loc='upper right')
plt.show()

accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
plt.plot(accuracy)
plt.plot(val_accuracy)
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train_accuracy', 'Val_accuracy'], loc='upper right')
plt.show()